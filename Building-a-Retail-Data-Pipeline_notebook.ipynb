{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe49dc-cda5-4d22-bb10-49e94cdb6437",
   "metadata": {
    "customType": "sql",
    "dataFrameVariableName": "store_df",
    "executionCancelledAt": null,
    "executionTime": 3748,
    "lastExecutedAt": 1699504035192,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "-- Write your SQL query here\nSELECT * FROM grocery_sales",
    "outputsMetadata": {
     "0": {
      "height": 344,
      "type": "dataFrame"
     }
    },
    "sqlCellMode": "dataFrame",
    "sqlSource": {
     "integrationId": "89e17161-a224-4a8a-846b-0adc0fe7a4b1",
     "type": "integration"
    }
   },
   "outputs": [],
   "source": [
    "-- Write your SQL query here\n",
    "SELECT * FROM grocery_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b4c2e-25fc-4986-9af9-2718e8e95e6b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1699504035241,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required packages\nimport pandas as pd\nimport numpy as np\nimport logging\nimport os\n"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae7cfd-4aef-429b-90a0-73efa79eb6db",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 76,
    "lastExecutedAt": 1699504035317,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Read the extra data from the parquet file and merge the DataFrames using \"index\" column\ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n  \n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(store_df, \"extra_data.parquet\")"
   },
   "outputs": [],
   "source": [
    "# Read the extra data from the parquet file and merge the DataFrames using \"index\" column\n",
    "def extract(store_data, extra_data):\n",
    "    extra_df = pd.read_parquet(extra_data)\n",
    "    merged_df = store_data.merge(extra_df, on = \"index\")\n",
    "    return merged_df\n",
    "  \n",
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "merged_df = extract(store_df, \"extra_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660d94d-aada-4f94-960a-2f94b1693bf2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1699504035373,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the extract() function with two parameters: one for the store data and the other one for the extra data\n# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n  # Fill NaNs using mean since we are dealing with numeric columns\n  # Set inplace = True to do the replacing on the current DataFrame\n    raw_data.fillna(\n      {\n          'CPI': raw_data['CPI'].mean(),\n          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n          'Unemployment': raw_data['Unemployment'].mean(),\n      }, inplace = True\n    )\n    # Define the type of the \"Date\" column and its format\n    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n\n    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n    \n    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n    return raw_data\n\n# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)\n\n# Create the avg_monthly_sales function that takes in the cleaned data from the last step\ndef avg_monthly_sales(clean_data):\n  \t# Select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis\n    holidays_sales = clean_data[[\"Month\", \"Weekly_Sales\"]]\n   \t# Create a chain operation with groupby(), agg(), reset_index(), and round() functions\n    # Group by the \"Month\" column and calculate the average monthly sales\n    # Call reset_index() to start a new index order\n    # Round the results to two decimal places\n    \n    holidays_sales = (holidays_sales.groupby(\"Month\")\n    .agg(Avg_Sales = (\"Weekly_Sales\", \"mean\"))\n    .reset_index().round(2))\n    return holidays_sales\n\n# Call the avg_monthly_sales() function and pass the cleaned DataFrame\nagg_data = avg_monthly_sales(clean_data)\n\n# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n    full_data.to_csv(full_data_file_path, index = False)\n    agg_data.to_csv(agg_data_file_path, index = False)"
   },
   "outputs": [],
   "source": [
    "# Create the extract() function with two parameters: one for the store data and the other one for the extra data\n",
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data):\n",
    "  # Fill NaNs using mean since we are dealing with numeric columns\n",
    "  # Set inplace = True to do the replacing on the current DataFrame\n",
    "    raw_data.fillna(\n",
    "      {\n",
    "          'CPI': raw_data['CPI'].mean(),\n",
    "          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n",
    "          'Unemployment': raw_data['Unemployment'].mean(),\n",
    "      }, inplace = True\n",
    "    )\n",
    "    # Define the type of the \"Date\" column and its format\n",
    "    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n",
    "    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n",
    "    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n",
    "\n",
    "    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n",
    "    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n",
    "    \n",
    "    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n",
    "    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \n",
    "                              \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n",
    "    return raw_data\n",
    "\n",
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(merged_df)\n",
    "\n",
    "# Create the avg_monthly_sales function that takes in the cleaned data from the last step\n",
    "def avg_monthly_sales(clean_data):\n",
    "  \t# Select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis\n",
    "    holidays_sales = clean_data[[\"Month\", \"Weekly_Sales\"]]\n",
    "   \t# Create a chain operation with groupby(), agg(), reset_index(), and round() functions\n",
    "    # Group by the \"Month\" column and calculate the average monthly sales\n",
    "    # Call reset_index() to start a new index order\n",
    "    # Round the results to two decimal places\n",
    "    \n",
    "    holidays_sales = (holidays_sales.groupby(\"Month\")\n",
    "    .agg(Avg_Sales = (\"Weekly_Sales\", \"mean\"))\n",
    "    .reset_index().round(2))\n",
    "    return holidays_sales\n",
    "\n",
    "# Call the avg_monthly_sales() function and pass the cleaned DataFrame\n",
    "agg_data = avg_monthly_sales(clean_data)\n",
    "\n",
    "# Create the load() function that takes in the cleaned DataFrame and the \n",
    "# aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n",
    "    full_data.to_csv(full_data_file_path, index = False)\n",
    "    agg_data.to_csv(agg_data_file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b693a08-984d-46ff-b91c-18079e39c5e5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 363,
    "lastExecutedAt": 1699504035737,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \nload(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")\n\n# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n  \t# Use the \"os\" package to check whether a path exists\n    file_exists = os.path.exists(file_path)\n    # Raise an exception if the path doesn't exist, hence, if there is no file found on a given path\n    if not file_exists:\n        raise Exception(f\"There is no file at the path {file_path}\")\n\n# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation(\"clean_data.csv\")\nvalidation(\"agg_data.csv\")"
   },
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n",
    "load(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")\n",
    "\n",
    "# Create the validation() function with one parameter: \n",
    "# file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "  \t# Use the \"os\" package to check whether a path exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    # Raise an exception if the path doesn't exist, hence, if there is no file found on a given path\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"There is no file at the path {file_path}\")\n",
    "\n",
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation(\"clean_data.csv\")\n",
    "validation(\"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982603d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
